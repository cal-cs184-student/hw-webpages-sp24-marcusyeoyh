<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">Marcus Yeo</h2>

<!-- Add Website URL -->
<h2 align="middle">Website URL: <a href="https://cal-cs184-student.github.io/hw-webpages-sp24-marcusyeoyh/hw3/index.html">https://cal-cs184-student.github.io/hw-webpages-sp24-marcusyeoyh/hw3/index.html</a></h2>

<br>
<h2 align="middle">Overview</h2>
<p>
    YOUR RESPONSE GOES HERE
</p>
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
<!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

<h3>
  Walk through the ray generation and primitive intersection parts of the rendering pipeline.
</h3>
<h4>
  Ray Generation
</h4>
<p>
    In task 1, I first converted hFov and vFov from degrees into radians, which was then used to calculate the top right x and y coordinates of the camera space using tan(0.5*hFov) and tan(0.5*vFov), referred to as <i>TopX</i> and <i>TopY</i>. Since we are given coordinates in the image space [0,1] X [0,1], we need to convert them into the camera space. This was done by normalizing the x and y values by deducting 0.5 from each coordinate before multiplying the resultant value by 2*TopX for the x coordinate and 2*TopY for the y coordinate. This gives us an accurate conversion from the image space to the camera space, adding a Z coordinate of -1 since we are now in 3D space. Since the camera origin is at (0,0,0), the point we have calculated is the direction vector of the ray. We then multiply with the c2w matrix provided to transform this direction vector into the world space.
</p>
<p>
  Thereafter, in task 2, I implemented <code>raytrace_pixel()</code>, where <i>ns_aa</i> samples were taken given a input pixel using <code>gridSampler->get_sample()</code>. The sample coordinates were then normalized to image space before being passed into the function in Task 1 to obtain a sample ray. I then estimated the radiance of the ray using <code>est_radiance_global_illumination()</code> and used Monte Carlo Integration to find the final value of each pixel.
</p>

<h4>
  Primitive Intersection: Ray-Triangle Intersection
</h4>
<p>
  To test for triangle intersection in <code>has_intersection()</code>, I implemnted the MÃ¶ller-Trumbore ray-triangle intersection algorithm taught in lecture to find the values of t, b1 and b2. I then checked if the t value was within <code>r.min_t</code> and <code>r.max_t</code>, as well as if the values of b1, b2 and 1-b1-b2 were within 0 and 1 inclusive. If all the conditions were met, this means that there is a intersection between the ray and triangle, and I return true and update <code>r.max_t</code> with the t value, if not, I return false.
</p>
<p>
  Implementation of <code>intersect()</code> was exactly the same as <code>has_intersection()</code>, except when a point of intersection was found between the ray and the triangle, I would populate the <code>Intersection *isect</code> structure with the necessary information, calculating the surface normal at the intersection using barycentric coordinates and the provided vertex normals of the triangle. I once again update <code>r.max_t</code> with the t value if a point of intersection is found.
</p>

<h4>
  Primitive Intersection: Ray-Sphere Intersection
</h4>
<p>
  Using the formula from the lectures, i implemented <code>test()</code>, to test if a ray intersected a given sphere and updated the provided parameters <code>t1</code> and <code>t2</code> with the calculated t values if a intersection was found, with <code>t1</code> being the smaller value and <code>t2</code> being the larger. Using the discriminant formula, I was able to check if there was one intersection (D==0), two intersections (D>0) or no intersections (D<0) and return true or false, as well as update the t1 and t2 values accordingly.
</p>
<p>
  <code>has_intersection()</code> was then implemented by calling the <code>test()</code> function, and if a intersection was found, the <code>r.max_t</code> value would be updated to <code>t1</code>, since t1 is the smaller value.
</p>
<p>
  <code>intersection()</code> was similarly implmented using <code>test()</code> and if a valid intersection was found, the <code>Intersection *isect</code> structure was updated, with the surface normal of the sphere computed using the normalized vector pointing from the sphere center to the intersection point.
</p>
<br>

<h3>
  Show images with normal shading for a few small .dae files.
</h3>
<!-- Example of including multiple figures -->
<div align="center">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="media/Part 1/bench.png" align="middle" width="400px"/>
        <figcaption>bench.dae</figcaption>
      </td>
      <td>
        <img src="media/Part 1/CBbunny.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
      <td>
        <img src="media/Part 1/CBspheres.png" align="middle" width="400px"/>
        <figcaption>CBspheres.dae</figcaption>
      </td>
    </tr>
  </table>
</div>


<h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
<!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

<h3>
  Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
</h3>
<p>
    For the given Start and End iterators, I ran through all the primitives to be included in the BVH and got their BBoxes and expanded them into a large BBox variable <code>bbox</code>. I then created a new BVHNode and assigned the bbox variable to it, as well as the start and end pointers with the provided Start and End iterators.
</p>

<p>
  I then checked if the number of primitives between the Start and End iterators were equal or less than the <code>max_leaf_size</code>. If they were, that would mean we have found a leaf node and the function can terminate after assigning the left and right pointers as NULL.
</p>

<p>
  If there are more primitives in the bounding box than <code>max_leaf_size</code>, then we would need to find a suitable axis to split the bounding box into. The heuristic I chose was to find the mean centroid position of all the bounding boxes of the primitives. The mean centroid position was chosen instead of the median as it is a lot more efficient to find the mean as compared to the median position, which involves sorting. Thereafter, I iterated through all the primitives again to find the variance of each axis according to the distribution of the primitives. I chose the axis with the most variance to split along. This was done as the axis with more variance would mean a more definitive seperation of the primitives, resulting in more efficient ray-bounding box tests as splitting along an axis with the least variance would mean more overlap in the resultant bounding boxes and hence more costly ray-bounding box tests.
</p>
<p>
  After an axis was chosen, I sorted the primitives based on their axis positions and split them along the mean centroid position, gaining a middle iterator Middle. I then recursively called <code>construct_bvh</code> for the left and right child of the current BVHNode where the primitives are partitioned from start to middle for the left node and middle to end for the right node.
</p>

<h3>
  Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="media/Part 2/walle.png" align="middle" width="400px"/>
        <figcaption>wall-e.dae</figcaption>
      </td>
      <td>
        <img src="media/Part 2/maxplanck.png" align="middle" width="400px"/>
        <figcaption>maxplanck.dae</figcaption>
      </td>
      <td>
        <img src="media/Part 2/CBdragon.png" align="middle" width="400px"/>
        <figcaption>CBdragon.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
</h3>
<p>
    When comparing rendering times on scenes with complex geometries, the implementation of BVH acceleration has tremendously sped up rendering times. For example, the Wall-E graphic, which took 346.5125 seconds to render with brute force took only 0.0357 seconds to render with BVH Acceleration. This is also shown in the CBdragon rendering times, where without BVH acceleration, it took 109.9390 seconds to render and only 0.0357 seconds to render with acceleration.
</p>
<br>

<h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

<h3>
  Walk through both implementations of the direct lighting function.
</h3>
<p>
    YOUR RESPONSE GOES HERE
</p>

<h3>
  Show some images rendered with both implementations of the direct lighting function.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>example1.dae</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>example1.dae</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>example2.dae</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>example2.dae</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>
<br>

<h3>
  Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, <b>not</b> uniform hemisphere sampling.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="200px"/>
        <figcaption>1 Light Ray (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="200px"/>
        <figcaption>4 Light Rays (example1.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="200px"/>
        <figcaption>16 Light Rays (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="200px"/>
        <figcaption>64 Light Rays (example1.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
    YOUR EXPLANATION GOES HERE
</p>
<br>

<h3>
  Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
</h3>
<p>
    YOUR RESPONSE GOES HERE
</p>
<br>


<h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

<h3>
  Walk through your implementation of the indirect lighting function.
</h3>
<p>
    YOUR RESPONSE GOES HERE
</p>
<br>

<h3>
  Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>example1.dae</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>example2.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>Only direct illumination (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>Only indirect illumination (example1.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    YOUR EXPLANATION GOES HERE
</p>
<br>

<h3>
  For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    YOUR EXPLANATION GOES HERE
</p>
<br>

<h3>
  Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>1 sample per pixel (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>2 samples per pixel (example1.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>4 samples per pixel (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>8 samples per pixel (example1.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>16 samples per pixel (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>64 samples per pixel (example1.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>1024 samples per pixel (example1.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    YOUR EXPLANATION GOES HERE
</p>
<br>


<h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

<h3>
  Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
</h3>
<p>
    YOUR RESPONSE GOES HERE
</p>
<br>

<h3>
  Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>Rendered image (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (example1.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>Rendered image (example2.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (example2.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


</body>
</html>
