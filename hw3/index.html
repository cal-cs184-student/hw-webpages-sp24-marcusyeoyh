<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">Marcus Yeo</h2>

<!-- Add Website URL -->
<h2 align="middle">Website URL: <a href="https://cal-cs184-student.github.io/hw-webpages-sp24-marcusyeoyh/hw3/index.html">https://cal-cs184-student.github.io/hw-webpages-sp24-marcusyeoyh/hw3/index.html</a></h2>

<br>
<h2 align="middle">Overview</h2>
<p>
    In this project, I implemented functions to create a efficient and accurate path tracer. I implemented ray generation, primitive intersection, bounding volume hierarchy, direct illumination, indirect illumination and global illumination. I also implemented Russian Roulette to terminate the recursion of the path tracer. I then tested the path tracer on various .dae files and compared the results with and without BVH acceleration, as well as with different sample rates and max_ray_depth values.
</p>
<p>
  This project was especially interesting to me as it allowed me to understand the inner workings of a path tracer and how it is able to create realistic images of scenes with complex geometries and light sources. I was able to understand the importance of BVH acceleration and how it is able to speed up the rendering process by a significant amount. I also learned about the importance of light sampling and how it is able to create more realistic images as compared to uniform hemisphere sampling.
</p>
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
<!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

<h3>
  Walk through the ray generation and primitive intersection parts of the rendering pipeline.
</h3>
<h4>
  Ray Generation
</h4>
<p>
    In task 1, I first converted hFov and vFov from degrees into radians, which was then used to calculate the top right x and y coordinates of the camera space using tan(0.5*hFov) and tan(0.5*vFov), referred to as <i>TopX</i> and <i>TopY</i>. Since we are given coordinates in the image space [0,1] X [0,1], we need to convert them into the camera space. This was done by normalizing the x and y values by deducting 0.5 from each coordinate before multiplying the resultant value by 2*TopX for the x coordinate and 2*TopY for the y coordinate. This gives us an accurate conversion from the image space to the camera space, adding a Z coordinate of -1 since we are now in 3D space. Since the camera origin is at (0,0,0), the point we have calculated is the direction vector of the ray. We then multiply with the c2w matrix provided to transform this direction vector into the world space.
</p>
<p>
  Thereafter, in task 2, I implemented <code>raytrace_pixel()</code>, where <i>ns_aa</i> samples were taken given a input pixel using <code>gridSampler->get_sample()</code>. The sample coordinates were then normalized to image space before being passed into the function in Task 1 to obtain a sample ray. I then estimated the radiance of the ray using <code>est_radiance_global_illumination()</code> and used Monte Carlo Integration to find the final value of each pixel.
</p>

<h4>
  Primitive Intersection: Ray-Triangle Intersection
</h4>
<p>
  To test for triangle intersection in <code>has_intersection()</code>, I implemnted the MÃ¶ller-Trumbore ray-triangle intersection algorithm taught in lecture to find the values of t, b1 and b2. I then checked if the t value was within <code>r.min_t</code> and <code>r.max_t</code>, as well as if the values of b1, b2 and 1-b1-b2 were within 0 and 1 inclusive. If all the conditions were met, this means that there is a intersection between the ray and triangle, and I return true and update <code>r.max_t</code> with the t value, if not, I return false.
</p>
<p>
  Implementation of <code>intersect()</code> was exactly the same as <code>has_intersection()</code>, except when a point of intersection was found between the ray and the triangle, I would populate the <code>Intersection *isect</code> structure with the necessary information, calculating the surface normal at the intersection using barycentric coordinates and the provided vertex normals of the triangle. I once again update <code>r.max_t</code> with the t value if a point of intersection is found.
</p>

<h4>
  Primitive Intersection: Ray-Sphere Intersection
</h4>
<p>
  Using the formula from the lectures, i implemented <code>test()</code>, to test if a ray intersected a given sphere and updated the provided parameters <code>t1</code> and <code>t2</code> with the calculated t values if a intersection was found, with <code>t1</code> being the smaller value and <code>t2</code> being the larger. Using the discriminant formula, I was able to check if there was one intersection (D==0), two intersections (D>0) or no intersections (D<0) and return true or false, as well as update the t1 and t2 values accordingly.
</p>
<p>
  <code>has_intersection()</code> was then implemented by calling the <code>test()</code> function, and if a intersection was found, the <code>r.max_t</code> value would be updated to <code>t1</code>, since t1 is the smaller value.
</p>
<p>
  <code>intersection()</code> was similarly implmented using <code>test()</code> and if a valid intersection was found, the <code>Intersection *isect</code> structure was updated, with the surface normal of the sphere computed using the normalized vector pointing from the sphere center to the intersection point.
</p>
<br>

<h3>
  Show images with normal shading for a few small .dae files.
</h3>
<!-- Example of including multiple figures -->
<div align="center">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="media/Part 1/bench.png" align="middle" width="400px"/>
        <figcaption>bench.dae</figcaption>
      </td>
      <td>
        <img src="media/Part 1/CBbunny.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
      <td>
        <img src="media/Part 1/CBspheres.png" align="middle" width="400px"/>
        <figcaption>CBspheres.dae</figcaption>
      </td>
    </tr>
  </table>
</div>


<h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
<!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

<h3>
  Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
</h3>
<p>
    For the given Start and End iterators, I ran through all the primitives to be included in the BVH and got their BBoxes and expanded them into a large BBox variable <code>bbox</code>. I then created a new BVHNode and assigned the bbox variable to it, as well as the start and end pointers with the provided Start and End iterators.
</p>

<p>
  I then checked if the number of primitives between the Start and End iterators were equal or less than the <code>max_leaf_size</code>. If they were, that would mean we have found a leaf node and the function can terminate after assigning the left and right pointers as NULL.
</p>

<p>
  If there are more primitives in the bounding box than <code>max_leaf_size</code>, then we would need to find a suitable axis to split the bounding box into. The heuristic I chose was to find the mean centroid position of all the bounding boxes of the primitives. The mean centroid position was chosen instead of the median as it is a lot more efficient to find the mean as compared to the median position, which involves sorting. Thereafter, I iterated through all the primitives again to find the variance of each axis according to the distribution of the primitives. I chose the axis with the most variance to split along. This was done as the axis with more variance would mean a more definitive seperation of the primitives, resulting in more efficient ray-bounding box tests as splitting along an axis with the least variance would mean more overlap in the resultant bounding boxes and hence more costly ray-bounding box tests.
</p>
<p>
  After an axis was chosen, I sorted the primitives based on their axis positions and split them along the mean centroid position, gaining a middle iterator Middle. I then recursively called <code>construct_bvh</code> for the left and right child of the current BVHNode where the primitives are partitioned from start to middle for the left node and middle to end for the right node.
</p>

<h3>
  Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="media/Part 2/walle.png" align="middle" width="400px"/>
        <figcaption>wall-e.dae</figcaption>
      </td>
      <td>
        <img src="media/Part 2/maxplanck.png" align="middle" width="400px"/>
        <figcaption>maxplanck.dae</figcaption>
      </td>
      <td>
        <img src="media/Part 2/CBdragon.png" align="middle" width="400px"/>
        <figcaption>CBdragon.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
</h3>
<p>
    When comparing rendering times on scenes with complex geometries, the implementation of BVH acceleration has tremendously sped up rendering times. For example, the Wall-E graphic, which took 346.5125 seconds to render with brute force took only 0.0357 seconds to render with BVH Acceleration. This is also shown in the CBdragon rendering times, where without BVH acceleration, it took 109.9390 seconds to render and only 0.0357 seconds to render with acceleration.
</p>
<br>

<h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

<h3>
  Walk through both implementations of the direct lighting function.
</h3>
<h4>
  Uniform Hemisphere Sampling
</h4>
<p>
    In order to get the lighting at a particular point, we need to first measure the amount of irradiance that falls on that point from surrounding light sources. We then scale the irradiance by the BSDF variable <code>f</code> in order to get the radiance that is going towards the camera at a given point.
</p>
<p>
  To estimate the irradiance on a point using uniform hemisphere sampling, we first need to take <code>num_samples</code> samples of incoming solid angles over the unit hemisphere. This was done by calling the <code>get_sample()</code> function of the <code>hemisphereSampler</code> variable of the pathtracer and is given a variable name <code>w_in</code>. It is important to note that this direction vector is in the object space. For each sample, we obtain the <code>costheta</code> value by finding the dot product between <code>w_in</code> and the Z axis as well as the BSDF value from the <code>isect</code> input variable. I then constructed a Ray object <code>nRay</code> and assigned the origin and direction vector to <code>hit_p</code> and <code>o2w*w_in</code> respetively, remembering to convert the <code>w_in</code> variable from object space to world space, as well as setting the <code>min_t</code> value as <code>EPS_F</code>. I then created a new Intersecion variable <code>nI</code> and if <code>costheta</code> was >0 (meaning direction sample is in front of the surface and not behind) and there is an intersect between <code>nRay</code> and a primitive in the bvh, I would save the emmission from that closest primitive to a vector L, before multiplying it by the BSDF value and <code>costheta</code> and adding it to <code>L_out</code> as the iterative step of the reflection equation.
</p>
<p>
  After all samples are finished, we then complete the reflection equation by multiplying <code>L_out</code> with 2PI and dividing by <code>num_samples</code>, since the sampling of solid angles was from a uniform distribution and a hemisphere has a possible 2PI solid angles to choose from. <code>L_out</code> is then returned.
</p>
<h4>
  Importance Sampling Lights
</h4>
<p>
  The algorithm for light sampling is quite similar to the unit hemisphere sampling algorithm. However instead of taking number of lights * <code>ns_area_light</code> uniform samples from a hemisphere, we will iterate through all the lights in the scene and if it is a point light source, we only take one sample, and if it is an area light source, we take <code>ns_area_light</code> samples using the <code>i->sample_L()</code> for each light i. This provides us with the necessary emission, <code>w_i</code>, distance to light and <code>pdf</code> values we need. We then construct a ray again with <code>hit_p</code> and <code>w_i</code>, and set the <code>min_t</code> of the ray as <code>EPS_F</code> and <code>max_t</code> as <code>distToLight - EPS_F</code>. We then similarly test for intersection between the point and the light source, however in this case if we have an intersecion, it means the light is blocked and we do not add the emmission value to the sample total. If there are no intersections, we add the iterative term of the reflection equation to <code>curL</code>, which includes the <code>pdf</code> variable this time since we are no longer doing uniform sampling. We then add each Monte Carlo Estimate for each light source to <code>L_out</code> and return it after all lights have been processed.
</p>

<h3>
  Show some images rendered with both implementations of the direct lighting function.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="media/Part 3/CBbunny_H_64_32.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
      <td>
        <img src="media/Part 3/bunny_64_32.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="media/Part 3/CBgems_H_64_32.png" align="middle" width="400px"/>
        <figcaption>CBgems.dae</figcaption>
      </td>
      <td>
        <img src="media/Part 3/gems_64_32.png" align="middle" width="400px"/>
        <figcaption>CBgems.dae</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>
<br>

<h3>
  Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, <b>not</b> uniform hemisphere sampling.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="media/Part 3/1ray_coil_64_32.png" align="middle" width="200px"/>
        <figcaption>1 Light Ray (example1.dae)</figcaption>
      </td>
      <td>
        <img src="media/Part 3/4ray_coil_64_32.png" align="middle" width="200px"/>
        <figcaption>4 Light Rays (example1.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="media/Part 3/16ray_coil_64_32.png" align="middle" width="200px"/>
        <figcaption>16 Light Rays (example1.dae)</figcaption>
      </td>
      <td>
        <img src="media/Part 3/64ray_coil_64_32.png" align="middle" width="200px"/>
        <figcaption>64 Light Rays (example1.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
    We can observe that there is much more noise when the number of rays is low (1 and 4) as compared to when the number of rays are higher (16 and 64 rays). This is because as the number of rays projected increases, we will get a better estimate from the Monte Carlo Estimator, since variance of the estimate falls as number of samples increases. Hence the more rays we project, the better the range of light and surface interaction is accounted for.
</p>
<br>

<h3>
  Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
</h3>
<p>
    From the renders, we can tell that the uniform hemisphere sampling method is more noisy and seems darker as compared to the light sampling. This is because sampling randomly from a hemisphere means that more of then than not the solid angles sampled will not point towards a light source, especially when there are few light sources or if the light source has a small surface area. This means the intersection tests carried out is less likely to hit a light source and return the emmission from it. This will affect the estimate of the irradiance on a point, especially if coupled with a low sample count.
</p>
<p>
  Since importance sampling only samples from light sources, which directly contributes to the amount of irradiance on a point, it allows for better estimation, since we are simply checking if a sample direction from the point to a particular light source is blocked or not.
</p>
<br>


<h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

<h3>
  Walk through your implementation of the indirect lighting function.
</h3>
<p>
    Firstly I edited the <code>raytrace_pixel</code> function to set <code>r.depth</code> for every ray created as <code>max_ray_depth</code>. In the <code>at_least_one_bounce_radiance()</code> function itself, I first checked if <code>isAccumBounces</code> was true. If it is, we add the <code>one_bounce_radiance()</code> of the point to L_out and then conduct Global Illumination with Russian Roulette, with termination probability set at 0.35. If the coin flip succeeds, the code is similar to Part 3, where we use <code>sample_f</code> to get a sample <code>wi</code> direction vector and <code>pdf</code> value. We then similarly construct a Ray object and set the depth to <code>r.depth - 1</code> and if the <code>r.depth</code> value is above 1, it means we still have more bounces to compute and we enter the recursive step by calling <code>at_least_one_bounce_radiance()</code> again. If the <code>r.depth</code> value is 1 or we hit a tails on the coin flip, we terminate the recursion and return the <code>one_bounce_radiance()</code> value. When returning from the recursion, we multiply the incoming radiance by the BSDF value, <code>costheta</code> value and divide by <code>pdf</code> and <code>cpdf</code> and add it to <code>L_out</code> as the iterative step of the reflection equation.
</p>
<p>
  If <code>isAccumBounces</code> is false, we similarly traverse recursively until we reach the <code>max_ray_depth</code> and return the <code>one_bounce_radiance()</code> value. For every recursive step, we again multiply the incoming radiance by the BSDF value, <code>costheta</code> value and divide by <code>pdf</code> and add it to <code>L_out</code> as the iterative step of the reflection equation.
</p>
<br>

<h3>
  Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="media/Part 4/bunny1024.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
      <td>
        <img src="media/Part 4/spheres1024.png" align="middle" width="400px"/>
        <figcaption>CBspheres.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="media/Part 4/gems_onebounce.png" align="middle" width="400px"/>
        <figcaption>Only direct illumination (CBgems.dae)</figcaption>
      </td>
      <td>
        <img src="media/Part 4/gems_morethanonebounce.png" align="middle" width="400px"/>
        <figcaption>Only indirect illumination (CBgems.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<h4>
  Only Direct Illumination
</h4>
<p>
    Direct Illumination only accounts for Single-Bounce illumination, which are light rays that come off the light source, hit a surface and then reach the camera. This gives us the most pronounced features of the final image, such as the shadows and color of the walls. The light source is not visible since light rays directly from the light source are under Zero-Bounce illumination and are not accounted for in the direct illumination.
</p>
<h4>
  Only Indirect Illumination
</h4>
<p>
  Indirect Illumination accounts for all N-1 bounces of light rays after First Bounce Illumination, with N being dependent on what we set <code>max_ray_depth</code> to. These are light rays that have bounced off 2 or more surfaces before reaching the camera, with their end radiance being the result of many applications of the reflection equation, depending on how many bounces the light ray has taken. 
</p>
<p>
  We can see that Indirect Illumination creates a somewhat back-lit appearance to the scene, allowing us to see features like the ceiling of the box, which because is parallel to the light source, is not visible in the Direct Illumination image. Additionally, we can see that shadows are less pronounced due to the light rays being reflected off surfaces and landing on spots that are directly under the light source. Normally these spots would be completely black as observed in Direct Illumination, however Indirect Illumination accounts for reflected light rays that land on these covered surfaces and when combined with Direct Illumination, creates a softer shadow which is more consistent with what we see in the real world, rather than a completely black shadow.
</p>
<br>

<h3>
  For CBbunny.dae, render the mth bounce of light with max_ray_depth set to 0, 1, 2, 3, 4, and 5 (the -m flag), and isAccumBounces=false. Explain in your writeup what you see for the 2nd and 3rd bounce of light, and how it contributes to the quality of the rendered image compared to rasterization. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="media/Part 4/bunny_m0_o0.png" align="middle" width="400px" />
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="media/Part 4/bunny_m1_o0.png" align="middle" width="400px" />
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="media/Part 4/bunny_m2_o0.png" align="middle" width="400px" />
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      
      <td>
        <img src="media/Part 4/bunny_m3_o0.png" align="middle" width="400px" />
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="media/Part 4/bunny_m4_o0.png" align="middle" width="400px" />
        <figcaption>max_ray_depth = 4 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="media/Part 4/bunny_m5_o0.png" align="middle" width="400px" />
        <figcaption>max_ray_depth = 5 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
  In the 2nd and 3rd bounce of light, we can see that the image is getting darker progressively, where the shadows are less pronounced. This is because the 2nd and 3rd bounce of light are accounting for the light rays that are reflected off surfaces two and three times before reaching the camera, resulting in a brighter combined image with smoother shadows. The images are progressively darker due to the surfaces being made of diffuse material. Hence with every bounce, the BSDF of the surface being less than 1, will result in the radiance of the outgoing ray being less than the incoming ray, resulting in a darker image. 
</p>
<p>
  This is a stark contrast to rasterization, where only the light rays that are directly visible to the camera are accounted for, and light rays that are reflected off surfaces are not accounted for, resulting in a less realistic image. This is especially true for scenes with complex geometries and light sources, where the light rays that are reflected off surfaces are able to reach the camera and contribute to create a more vibrant and realistic final image.
</p>
<br>

<h3>
  For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, 4, and 5(the -m flag). Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="media/Part 4/bunny_m0_o1.png" align="middle" width="400px" />
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="media/Part 4/bunny_m1_o1.png" align="middle" width="400px" />
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="media/Part 4/bunny_m2_o1.png" align="middle" width="400px" />
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      
      <td>
        <img src="media/Part 4/bunny_m3_o1.png" align="middle" width="400px" />
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="media/Part 4/bunny_m4_o1.png" align="middle" width="400px" />
        <figcaption>max_ray_depth = 4 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="media/Part 4/bunny_m5_o1.png" align="middle" width="400px" />
        <figcaption>max_ray_depth = 5 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
  Here, we can see the accumulation of light rays that have at most N bounces depending on what max_ray_depth is set to. As we increase the max_ray_depth, the brightness of the image seems to increase and the shadows get less dark and pronounced. This is consistent with what happens in the real world as the more bounces of light we have, the more light is reflected and the more light is able to reach the camera.
</p>
<br>
<h3>
  For CBbunny.dae, output the Russian Roulette rendering with max_ray_depth set to 0, 1, 2, 3, 4, and 100(the -m flag). Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="media/Part 4/bunny1024_m0.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="media/Part 4/bunny1024_m1.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="media/Part 4/bunny1024_m2.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="media/Part 4/bunny1024_m3.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="media/Part 4/bunny1024_m100.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    As we can observe, as we increase the max_ray_depth, the brightness of the image seems to increase and the shadows get less dark and pronounced. This is consistent with what happens in the real world as the more bounces of light we have, the more light is reflected and the more light is able to reach the camera. max_ray_depth = 0 is a black image with only the light source visible since it only accounts for zero-bounce illumination, which are the rays of light coming directly into the camera from the light source. max_ray_depth = 1 accounts for both zero bounce and one bounce illumination, which produces very pronounced shadows if the light source is not directly visible. max_ray_depth > 1 accounts for indirect illumination, which is the light that is reflected off surfaces and then reaches the camera.
</p>
<br>

<h3>
  Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="media/Part 4/lucy_s1.png" align="middle" width="400px"/>
        <figcaption>1 sample per pixel (CBlucy.dae)</figcaption>
      </td>
      <td>
        <img src="media/Part 4/lucy_s2.png" align="middle" width="400px"/>
        <figcaption>2 samples per pixel (CBlucy.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="media/Part 4/lucy_s4.png" align="middle" width="400px"/>
        <figcaption>4 samples per pixel (CBlucy.dae)</figcaption>
      </td>
      <td>
        <img src="media/Part 4/lucy_s8.png" align="middle" width="400px"/>
        <figcaption>8 samples per pixel (CBlucy.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="media/Part 4/lucy_s16.png" align="middle" width="400px"/>
        <figcaption>16 samples per pixel (CBlucy.dae)</figcaption>
      </td>
      <td>
        <img src="media/Part 4/lucy_s64.png" align="middle" width="400px"/>
        <figcaption>64 samples per pixel (CBlucy.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="media/Part 4/lucy_s1024.png" align="middle" width="400px"/>
        <figcaption>1024 samples per pixel (CBlucy.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    As observed, as we increase the number of samples per pixel, we get a huge decrease in the amount of noise present in the image. The more samples we take, the better we can estimate the radiance at a given point, and the less variance there is in the estimate. This is especially important in scenes with complex geometries and lighting, where the amount of light and surface interaction is more varied and hence more difficult to estimate.
</p>
<br>


<h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

<h3>
  Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
</h3>
<p>
    Adaptive Sampling is a algorithm that allows us to sample more in regions of the image that are more noisy and sample less in regions that are less noisy. Since some pixels converge faster with low sampling rates while other pixels require more samples, we can dynamically adjust the number of samples per pixel based on the convergence value <code>I</code> of the pixel. Doing so reduces noise and speeds up rendering times.
</p>
<p>
  In <code>raytrace_pixel()</code>, I introduced two variables n and batch, which were used to keep track of the number of samples taken so far, and the number of samples in the current batch. For every sample, I checked if the <code>batch</code> value is equal to <code>samplesPerBatch</code>. If it is, we calculate values for <code>mu</code>, <code>var</code> and <code>I</code> using formulas given in the question. We then check if the <code>I</code> is smaller or equal to <code>maxTolerance*mu</code>. If it is, we can stop taking more samples and break the cycle, if not we will take more samples until <code>batch</code> value is equal to <code>samplesPerBatch</code> again.
</p> 
<p>
  While <code>batch != samplesPerBatch</code> the implementation of <code>raytrace_pixel()</code> is exactly the same, except where we find <code>float illum = estimate.illum()</code> and update s1 and s2 with the <code>illum</code> value as needed. After exiting the loop, we divide total by the number of samples taken n and update the samplebuffer. We also update <code>sampleCountBuffer[x + y * sampleBuffer.w] = n</code>.
</p>
<br>

<h3>
  Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="media/Part 4/bunny2048.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="media/Part 4/bunny2048_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="media/Part 4/walle2048.png" align="middle" width="400px"/>
        <figcaption>Rendered image (wall-e.dae)</figcaption>
      </td>
      <td>
        <img src="media/Part 4/walle2048_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (wall-e.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


</body>
</html>
